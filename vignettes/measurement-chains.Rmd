---
title: "Measurement Chains"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Measurement Chains}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

# Helper function to check if all strings are not empty
all_defined <- function(x) {
  all(sapply(x, stringr::str_length) > 0L)
}

# Get environment variables for access to SFTP server with input data
con <- kwb.geosalz:::get_environment_variables(
  server = "MESSKETTEN_SERVER", 
  user   = "MESSKETTEN_USER", 
  pw     = "MESSKETTEN_PASSWORD"
)

# Get environment variables for access to Nextcloud server
nc <- kwb.geosalz:::get_environment_variables(
  server = "NEXTCLOUD_URL", 
  user   = "NEXTCLOUD_USER", 
  pw     = "NEXTCLOUD_PASSWORD"
)

# Are all environment variables defined?
con_defined <- all_defined(con)
nc_defined <- all_defined(nc)

# Is this script running on a GitHub server?
is_ghactions <- identical(Sys.getenv("CI"), "true")

# Is this script running locally (with access to SFTP server)?
local <- con_defined & !is_ghactions
```

## Background 

One *measurement chain* consists of six sensors that are placed in one
production well, but at different filter depths. In total, three measurement
chains will be installed in three different production wells.

Table 1 below lists all individual sensors. The number of the sensor within the
well is given in column `sensor_endnummer`. The identifier of the production
well that the sensor is located in is given in column `brunnen_nummer`.

```{r, echo = FALSE}
metadata <- kwb.geosalz::get_measurementchains_metadata()
DT::datatable(metadata, caption = paste(
  "Table 1: Metadata on Measurement Chains. Each row represents a sensor."
))
```

Each sensor (type: *WLF05*, **TODO**: add-link-to-factsheet-on-kwb-cloud)
measures two parameters with the following characteristics:

- electrical conductivity:
    + measurement range: 0 - 20 mS
    + measurement range detection: automatic
    + typical accuracy: +/- 1.5 % of measurement value

- temperature:
    + measurement range: 0 - 50 `r cat("\u00B0")` Celsius
    + measurement resolution: 0.1 `r cat("\u00B0")` Celsius
    + typical accuracy: <= 0.1 `r cat("\u00B0")` Celsius

The maximum temporal resolution that a sensor can provide is "every second".
Within the GeoSalz project, measurements are taken every 5 minutes. This should
be sufficient to detect potential salinity shifts that are caused by changes in
the pumping regime.

## Data Management 

### Define Paths 

```{r define_paths,  eval = con_defined}
# Define paths to directories, using <placeholder> replacements
paths <- kwb.utils::resolve(list(
  
  # Local temporary directory
  local_dir = fs::path_abs(tempdir()),
  
  # Target directory for downloaded measurement chain files (.csv)
  download_dir = "<local_dir>/download", 
 
  # Local directory for aggregated data (.csv and .zip)
  export_dir = "<local_dir>/export", 
 
  # KWB cloud directory to which data in "export_dir" is uploaded
  upload_dir = "projects/GeoSalz/Monitoring/messketten"
))

# Print all paths
paths
```

### Define SFTP Login

Run `usethis::edit_r_environ()` to edit the `.Renviron` file. The file defines
environment variables that are to be made accessible during an R session. Add
the following three rows, defining three more environment variables, to the
file. The environment variables are required to log in to the SFTP server from
which input data are downloaded.

```
MESSKETTEN_SERVER=<sftp_url>
MESSKETTEN_USER=<sftp_username>
MESSKETTEN_PASSWORD=<sftp_userpassword>
```

Replace the placeholders `<sftp_url>`, `<sftp_username>`, `<sftp_userpassword>`
with the URL, the user name, and the password, respectively, that are required
to get access to the SFTP server.

Save the `.Renviron` file and restart the R session (e.g. with "Session/Restart
R" from the menu in RStudio) to make the environment variables available in R.

In case that the SFTP login credentials are correct, the code below should work.
It downloads the measurement chains data (i.e. parameters electrical
conductivity and temperature) from the SFTP server to a user-defined directory
on your local device.

### Data Download

```{r measurementchains_data-download,  eval = con_defined}
# Metadata of measurement chains (see also Table 1 above)
metadata <- kwb.geosalz::get_measurementchains_metadata()
str(metadata)

# Information on available measurement chain files on SFTP server
mc_files <- kwb.geosalz::get_measurementchains_files()

str(mc_files)
head(mc_files)

# Download measurement chain files (.csv) from SFTP server
csv_files <- kwb.geosalz::download_measurementchains_data(
  sftp_paths = mc_files$sftp_path,
  target_directory = paths$download_dir,
  debug = TRUE
)

# Print the paths to the downloaded files
csv_files
```

### Data Import

The following code imports the downloaded measurement chains files (.csv) into
R:

```{r measurementchains_data-import,  eval = con_defined}
# Stop in case that no csv file is available
stopifnot(nrow(csv_files) > 0L) 

# Import csv files using multiple CPU cores
mc_data <- kwb.geosalz::read_measurementchains_data(
  csv_files,
  run_parallel = TRUE,
  debug = TRUE
)
```

The following datasets were imported into R: 

```{r measurementchains_data-import_stats, eval = con_defined}
mc_data_stats <- mc_data %>%
  kwb.geosalz::get_measurmentchains_data_stats() %>% 
  dplyr::arrange(
    .data$parameter, 
    dplyr::desc(.data$sensor_id)
  )
```

```{r measurementchains_data-import_stats_table, echo = local, eval = local }
DT::datatable(mc_data_stats)
```

These cover the time period from `r min(mc_data_stats$datetime_min)` to
`r max(mc_data_stats$datetime_max)` with a total of 
`r sum(mc_data_stats$number_of_samples)` samples.

### Data Export

```{r measurementchains_data-export, eval = con_defined}
debug <- TRUE

# Export "mc_data" to csv file
data_csv_path <- kwb.geosalz::write_measurementchains_data(
  mc_data,
  target_directory = paths$export_dir,
  to_zip = FALSE,
  debug = debug
)

size_data_csv <- fs::file_size(data_csv_path)

# Export "mc_data" to zip file (~10x less disk space for test dataset)
data_zip_path <- kwb.geosalz::write_measurementchains_data(
  mc_data,
  target_directory = paths$export_dir,
  to_zip = TRUE,
  debug = debug
)

size_data_zip <- fs::file_size(data_zip_path)
size_data_csv / size_data_zip

extract_data_timeperiod <- function(data_path) {
  basename(data_path) %>% 
    kwb.utils::replaceFileExtension("") %>%  
    stringr::str_remove("^mc[-|_]data")
}

# Define helper function to write a csv file in a target directory
write_csv <- function(df, postfix = "", target_dir) {
  fs::dir_create(target_dir)
  name <- deparse(substitute(df))
  file <- file.path(target_dir, paste0(name, postfix,  ".csv"))
  readr::write_csv(df, file)
  file
}

postfix <- extract_data_timeperiod(data_zip_path)
target_dir <- paths$export_dir

# Export data and metadata to csv files
stats_path <- write_csv(mc_data_stats, postfix, target_dir)
metadata_path <- write_csv(metadata, postfix, target_dir)
files_path <- write_csv(mc_files, postfix, target_dir)

# Define function that plots data to a pdf file
plot_to_pdf <- function(mc_data, para, target_dir = ".", debug = TRUE) {
  
  plots <- kwb.geosalz::plot_measurementchains(mc_data, para)
  
  sapply(names(plots), function(name) {
    path <-  file.path(target_dir, sprintf("mc_data_%s.pdf", name))
    kwb.utils::catAndRun(
      sprintf("Writting '%s' to '%s'", name, path),
      expr = {
        kwb.utils::preparePdf(path, width.cm = 25, height.cm = 15)
        print(plots[[name]])
        dev.off()
        path 
      }, 
      dbg = debug
    )
  })
}

pdf_files <- c(
  plot_to_pdf(mc_data, "Leitfaehigkeit", paths$export_dir), 
  plot_to_pdf(mc_data, "Temperatur", paths$export_dir)
)

data_zip_path
```

The file size of the `r basename(data_csv_path)` file is 
`r round(size_data_csv / size_data_zip, 1)` times larger compared to compressing 
it into `r basename(data_zip_path)`.

### Data Upload 

In order to automate the data management process the 
[exported data](#data-export) these data will be uploaded to a restricted 
shared folder `r paths$upload_directory` on the KWB cloud. For doing so the 
following environment variables need to be defined in case the code below should 
be run from a client computer. However, as these variables are defined in a 
GitHub actions workflow there is no need to do this locally.

```
NEXTCLOUD_URL = "https://<replace-with-kwb-cloud-url>"
NEXTCLOUD_USER = "<your-kwb-cloud-username>" # your username
NEXTCLOUD_PASSWORD = "your-nextcloud-app-password" ### see details below
```

Subsequently the following code is run for uploading the data:

```{r measurementchains_data-upload,  eval = con_defined & nc_defined}

# Upload all files in "paths$export_dir"
#upload_files <- list.files(paths$export_dir, full.names = TRUE)


# List paths of files to upload explicitly
upload_files <- c(
  stats_path, 
  pdf_files,
  metadata_path, 
  files_path,
  data_zip_path
)

for (file in upload_files) {
  
  if (!file.exists(file)) {
    message("File does not exist: ", file)
    next
  }
  
  kwb.utils::catAndRun(
    messageText = paste("Uploading file", file),
    expr = try(kwb.nextcloud::upload_file(
      file = file, 
      target_path = paths$upload_dir
    )),
    dbg = TRUE
  )
}
```

```{r measurementchains_data-delete, echo = FALSE, eval = con_defined & is_ghactions}
fs::file_delete(path = list.files(paths$export_dir, full.names = TRUE))
```
