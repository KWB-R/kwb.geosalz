---
title: "Measurement Chains"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Measurement Chains}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

# Helper function to check if all strings are not empty
all_defined <- function(x) {
  all(sapply(x, stringr::str_length) > 0L)
}

# Get environment variables for access to ftp server with input data
con <- kwb.geosalz:::get_environment_variables(
  server = "MESSKETTEN_SERVER", 
  user   = "MESSKETTEN_USER", 
  pw     = "MESSKETTEN_PASSWORD"
)

# Get environment variables for access to Nextcloud server
nc <- kwb.geosalz:::get_environment_variables(
  server = "NEXTCLOUD_URL", 
  user   = "NEXTCLOUD_USER", 
  pw     = "NEXTCLOUD_PASSWORD"
)

# Are all environment variables defined?
con_defined <- all_defined(con)
nc_defined <- all_defined(nc)

# Is this script running on a GitHub server?
is_ghactions <- identical(Sys.getenv("CI"), "true")

# Is this script running locally (with access to ftp server)?
local <- con_defined & !is_ghactions
```

## Background 

One *measurement chain* consists of six sensors that are placed in one
production well, but at different filter depths. In total, three measurement
chains will be installed in three different production wells.

Table 1 below lists all individual sensors. The number of the sensor within the
well is given in column `sensor_endnummer`. The identifier of the production
well that the sensor is located in is given in column `brunnen_nummer`.

```{r, echo = FALSE}
metadata <- kwb.geosalz::get_measurementchains_metadata()
DT::datatable(metadata, caption = "Table 1: Metadata of Measurement Chains")
```

Each sensor (type: `WLF05`, `link_to_manufacturer_factsheet`: `to_do:
add-link-to-factsheet-on-kwb-cloud`) measures the the following two parameters
with the characteristics summarized below:

- `electrical conductivity`: 

+ measurement range: `0 - 20 mS`
+ measurement range detection: `automatic`
+ typical accuracy: +/- 1.5 % of measurement value

- `temperature`:

+ measurement range: `0 - 50 \u00B0 Celsius`
+ measurement resolution: `0.1 \u00B0 Celsius`
+ typical accuracy: `<= 0.1 \u00B0 Celsius`

The maximum `temporal resolution` of each sensor is `every second`. Within the 
project `GeoSalz` a measurement `every 5 minutes` is proposed by KWB in order to 
be able to detect potential salinity shifts due to a switched pumping regime.

## Data Management 

### Define Paths 

```{r define_paths,  eval = con_defined}
paths_list <- list(
 local_dir = fs::path_abs(tempdir()),
 # Define target directory for download of measurement chain `.csv` files
 download_dir = "<local_dir>/download", 
 # Local directory for exporting imported data to ".csv" and ".zip" 
 export_dir = "<local_dir>/export", 
 # Upload KWB cloud directory for imported data in "export_dir"
 upload_dir = "projects/GeoSalz/Monitoring/messketten"
)

paths <- kwb.utils::resolve(paths_list)

paths
```


### Define SFTP Login

Run `usethis::edit_r_environ()` to edit the R `.Renviron` file and add the 
following three environment variables.

```
MESSKETTEN_SERVER = <sftp_url>
MESSKETTEN_USER = <sftp_username>
MESSKETTEN_PASSWORD = <sftp_userpassword>
```

Finally save the `.Renviron` file and restart `R(Studio)`. Subsequently the code
below will work (in case the SFTP login credentials were defined correctly!) and
assure that the `measurement chains` data (i.e. parameters `electrical
conductivity` and `temperature`) will be downloaded from the `SFTP server` to a
user-defined directory on your local device.



### Data Download

```{r measurementchains_data-download,  eval = con_defined}
library(kwb.geosalz)

### Metadata of measurement chains (see also Table 1 above)
metadata <- kwb.geosalz::get_measurementchains_metadata()
str(metadata)

### Tibble with Information on available measurement chain files on SFTP server
mc_files <- kwb.geosalz::get_measurementchains_files()

str(mc_files)
head(mc_files)

# Download of measurement chain `.csv` files from SFTP server
csv_files <- kwb.geosalz::download_measurementchains_data(
  sftp_paths = mc_files$sftp_path,
  target_directory = paths$download_dir,
  debug = TRUE
)

csv_files
```

### Data Import

The downloaded `.csv` measurement chains files will be imported into R by
running the code below:

```{r measurementchains_data-import,  eval = con_defined}
### print error in case no csv file is available
stopifnot(nrow(csv_files) > 0L) 

## CSV import using multiple CPU cores
mc_data <- kwb.geosalz::read_measurementchains_data(
  csv_files,
  run_parallel = TRUE,
  debug = TRUE
)


```

In a nutshell the following datasets were imported into R: 

```{r measurementchains_data-import_stats,  eval = con_defined}
mc_data_stats <- kwb.geosalz::get_measurmentchains_data_stats(mc_data) %>% 
  dplyr::arrange(.data$parameter, 
                 dplyr::desc(.data$sensor_id))
```

```{r measurementchains_data-import_stats_table, echo = local,  eval = local }
DT::datatable(mc_data_stats)
```

These cover the time period from `r min(mc_data_stats$datetime_min)` to
`r max(mc_data_stats$datetime_max)` with a total of 
`r sum(mc_data_stats$number_of_samples)` samples.

### Data Export

```{r measurementchains_data-export,  eval = con_defined}
debug <- TRUE

### Export "mc_data" to csv file
data_csv_path <- kwb.geosalz::write_measurementchains_data(
  mc_data,
  target_directory = paths$export_dir,
  to_zip = FALSE,
  debug = debug
)

size_data_csv <- fs::file_size(data_csv_path)

### Export "mc_data" to zip file (~10x less disk space for test dataset)
data_zip_path <- kwb.geosalz::write_measurementchains_data(
  mc_data,
  target_directory = paths$export_dir,
  to_zip = TRUE,
  debug = debug
)

size_data_zip <- fs::file_size(data_zip_path)
size_data_csv / size_data_zip

### Export "mc_files" and "metadata" to csv
extract_data_timeperiod <- function(data_path) {
  basename(data_path) %>% 
    kwb.utils::replaceFileExtension("") %>%  
    stringr::str_remove("^mc[-|_]data")
}

write_csv <- function(df, postfix = "", target_directory) {
  
fs::dir_create(target_directory)  
  
df_name <- deparse(substitute(df))

target_path <- file.path(target_directory, 
                         paste0(df_name, postfix,  ".csv"))

readr::write_csv(df, target_path)
target_path
}

stats_path <- write_csv(mc_data_stats,
                        postfix = extract_data_timeperiod(data_zip_path),
                        target_directory = paths$export_dir)

metadata_path <- write_csv(metadata, 
                           postfix = extract_data_timeperiod(data_zip_path),
                           target_directory = paths$export_dir)


files_path <- write_csv(mc_files, 
                        postfix = extract_data_timeperiod(data_zip_path),
                        target_directory = paths$export_dir)

plot_to_pdf <- function(mc_data, para, target_dir = ".", debug = TRUE) {
plts <- kwb.geosalz::plot_measurementchains(mc_data, para)

sapply(names(plts), function(name) {
path <-  file.path(target_dir, sprintf("mc_data_%s.pdf", name))
kwb.utils::catAndRun(messageText = sprintf("Writting '%s' to '%s'", 
                                           name, 
                                           path),
                     expr = {
                       kwb.utils::preparePdf(path, 
                                             width.cm = 25, 
                                             height.cm = 15)
                       print(plts[[name]])
                       dev.off()
                       path 
                       }, 
                     dbg = debug)
 })
}

pdf_files <- c(plot_to_pdf(mc_data, 
                           "Leitfaehigkeit", 
                           target_dir = paths$export_dir), 
               plot_to_pdf(mc_data, 
                           "Temperatur", 
                           target_dir = paths$export_dir))

data_zip_path
```

The file size of the `r basename(data_csv_path)` file is 
`r round(size_data_csv / size_data_zip, 1)` times larger compared to compressing 
it into `r basename(data_zip_path)`.

### Data Upload 

In order to automate the data management process the 
[exported data](#data-export) these data will be uploaded to a restricted 
shared folder `r paths$upload_directory` on the KWB cloud. For doing so the 
following environment variables need to be defined in case the code below should 
be run from a client computer. However, as these variables are defined in a 
GitHub actions workflow there is no need to do this locally.


```
NEXTCLOUD_URL = "https://<replace-with-kwb-cloud-url>"
NEXTCLOUD_USER = "<your-kwb-cloud-username>" # your username
NEXTCLOUD_PASSWORD = "your-nextcloud-app-password" ### see details below
```

Subsequently the following code is run for uploading the data:

```{r measurementchains_data-upload,  eval = con_defined & nc_defined}

# Upload all files in "paths$export_dir"
#upload_files <- list.files(paths$export_dir, full.names = TRUE)


# List paths of files to upload explicitly
upload_files <- c(
  stats_path, 
  pdf_files,
  metadata_path, 
  files_path,
  data_zip_path
)

for (upload_file in upload_files) {
  
  if (!file.exists(upload_file)) {
    message("File does not exist: ", upload_file)
    next
  }
  
  kwb.utils::catAndRun(
    messageText = paste("Uploading file", upload_file),
    expr = try(kwb.nextcloud::upload_file(
      file = upload_file, 
      target_path = paths$upload_dir
    )),
    dbg = TRUE
  )
}
```

```{r measurementchains_data-delete,  echo = FALSE, eval = con_defined & is_ghactions}

delete_files <- list.files(path = paths$export_dir, 
                           full.names = TRUE)

fs::file_delete(path = delete_files)
```

