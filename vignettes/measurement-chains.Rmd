---
title: "Measurement Chains"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Measurement Chains}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

# Load pipe operator
`%>%` <- magrittr::`%>%`

# Get environment variables for access to SFTP server with input data
con <- kwb.geosalz:::get_environment_variables(
  server = "MESSKETTEN_SERVER", 
  user   = "MESSKETTEN_USER", 
  pw     = "MESSKETTEN_PASSWORD"
)

# Get environment variables for access to Nextcloud server
nc <- kwb.geosalz:::get_environment_variables(
  server = "NEXTCLOUD_URL", 
  user   = "NEXTCLOUD_USER", 
  pw     = "NEXTCLOUD_PASSWORD"
)

# Are all environment variables defined?
con_defined <- kwb.geosalz:::all_defined(con)
nc_defined <- kwb.geosalz:::all_defined(nc)

# Is this script running on a GitHub server?
is_ghactions <- identical(Sys.getenv("CI"), "true")

# Is this script running locally (with access to SFTP server)?
local <- con_defined & !is_ghactions
```

## Background 

One *measurement chain* consists of six sensors that are placed in one
production well, but at different filter depths. In total, three measurement
chains will be installed in three different production wells.

Table 1 below lists all individual sensors. The number of the sensor within the
well is given in column `sensor_endnummer`. The identifier of the production
well that the sensor is located in is given in column `brunnen_nummer`.

```{r, echo = FALSE}
metadata <- kwb.geosalz::get_measurementchains_metadata()
DT::datatable(metadata, caption = paste(
  "Table 1: Metadata on Measurement Chains. Each row represents a sensor."
))
```

Each sensor (type: *WLF05*, **TODO**: add-link-to-factsheet-on-kwb-cloud)
measures two parameters with the following characteristics:

- electrical conductivity:
    + measurement range: 0 - 20 mS
    + measurement range detection: automatic
    + typical accuracy: +/- 1.5 % of measurement value

- temperature:
    + measurement range: 0 - 50 degree Celsius
    + measurement resolution: 0.1 Kelvin
    + typical accuracy: <= 0.1 Kelvin

The maximum temporal resolution that a sensor can provide is "every second".
Within the GeoSalz project, measurements are taken every 5 minutes. This should
be sufficient to detect potential salinity shifts that are caused by changes in
the pumping regime.

## Data Management 

### Define Paths 

```{r define_paths,  eval = con_defined}
# Define paths to directories, using <placeholder> replacements
paths <- kwb.utils::resolve(list(
  
  # Local temporary directory
  local_dir = kwb.geosalz:::temp_dir(),
  
  # Target directory for downloaded measurement chain files (.csv)
  download_dir = "<local_dir>/download", 
 
  # Local directory for aggregated data (.csv and .zip)
  export_dir = "<local_dir>/export", 
 
  # KWB cloud directory to which data in "export_dir" is uploaded
  upload_dir = "projects/GeoSalz/Monitoring/messketten",
  
  # KWB cloud directory with latest BWB well operation data
  well_operation = "<upload_dir>/BWB_Brunnen_Prozessdaten"
))

# Print all paths
paths
```

### Define SFTP Login

Run `usethis::edit_r_environ()` to edit the `.Renviron` file. The file defines
environment variables that are to be made accessible during an R session. Add
the following three rows, defining three more environment variables, to the
file. The environment variables are required to log in to the SFTP server from
which input data are downloaded.

```
MESSKETTEN_SERVER=<sftp_url>
MESSKETTEN_USER=<sftp_username>
MESSKETTEN_PASSWORD=<sftp_userpassword>
```

Replace the placeholders `<sftp_url>`, `<sftp_username>`, `<sftp_userpassword>`
with the URL, the user name, and the password, respectively, that are required
to get access to the SFTP server.

Save the `.Renviron` file and restart the R session (e.g. with "Session/Restart
R" from the menu in RStudio) to make the environment variables available in R.

In case that the SFTP login credentials are correct, the code below should work.
It downloads the measurement chains data (i.e. parameters electrical
conductivity and temperature) from the SFTP server to a user-defined directory
on your local device.

### Data Download

#### What files are available on the SFTP server?

```{r measurement-chains_data-download_1,  eval = con_defined}
# Metadata of measurement chains (see also Table 1 above)
metadata <- kwb.geosalz::get_measurementchains_metadata()
str(metadata)

# Information on available measurement chain files on SFTP server
mc_files <- kwb.geosalz::get_measurementchains_files()

str(mc_files)
head(mc_files)

# All paths to files on the SFTP server
all_paths <- kwb.utils::selectColumns(mc_files, "sftp_path")
```

#### What data are already available on the Nextcloud server?

```{r measurement-chains_data-download_2, eval = con_defined}
# Download existing data from the Nextcloud server. NULL is returned if there is
# no file "mc_data.zip" in the Nextcloud folder 
# "projects/GeoSalz/Monitoring/messketten"
old_data <- kwb.geosalz::get_measurement_chain_data_on_cloud()
#old_data <- NULL
```

#### What files need to be downloaded from the SFTP server?

```{r measurement-chains_data-download_3, eval = con_defined}
# Determine the SFTP paths of the files to be downloaded. Download all available
# files if there was no data on the Nextcloud yet. Otherwise, download only
# those files that have been added to the SFTP server since when this script was
# run the last time.
new_paths <- if (is.null(old_data)) {
  
  all_paths
  
} else {
  
  # Get SFTP paths from which data were originally retrieved
  old_paths <- unique(kwb.utils::selectColumns(old_data, "file"))
  
  # Determine the paths to the new files that need to be downloaded
  setdiff(all_paths, old_paths)
}
```

#### Download the new files from the SFTP server

```{r measurement-chains_data-download_4, eval = con_defined}
# If there are new files to download, download them.
csv_files <- if (length(new_paths)) {
  
  # Download only the new measurement chain files (.csv) from the SFTP server
  kwb.geosalz::download_measurementchains_data(
    sftp_paths = new_paths,
    target_directory = paths$download_dir,
    debug = TRUE
  )
  
} # else NULL implicitly

str(csv_files)
```

### Data Import

The following code imports the downloaded measurement chains files (.csv) into
R:

```{r measurementchains_data-import,  eval = con_defined}
# Set the current data to the old data (may be NULL)
mc_data <- old_data

# If new files have been downloaded, import them into R
if (length(csv_files)) {
  
  # Import csv files using multiple CPU cores
  new_data <- kwb.geosalz::read_measurementchains_data(
    csv_files,
    run_parallel = TRUE,
    debug = TRUE
  )
  
  # Combine the new data with the existing data and reorder
  mc_data <- mc_data %>%
    rbind(new_data) %>%
    kwb.geosalz::order_measurement_chain_data()
}
```

The following datasets were imported into R or already available in R: 

```{r measurementchains_data-import_stats, eval = con_defined}
mc_data_stats <- mc_data %>%
  kwb.geosalz::get_measurmentchains_data_stats() %>% 
  dplyr::arrange(
    .data$parameter, 
    dplyr::desc(.data$sensor_id)
  )
```

```{r measurementchains_data-import_stats_table, echo = local, eval = local }
DT::datatable(mc_data_stats)
```

These cover the time period from `r min(mc_data_stats$datetime_min)` to
`r max(mc_data_stats$datetime_max)` with a total of 
`r sum(mc_data_stats$number_of_samples)` samples.

### Data Export

```{r measurementchains_data-export, eval = con_defined}
debug <- TRUE

# Export "mc_data" to csv file
data_csv_path <- kwb.geosalz::write_measurementchains_data(
  mc_data,
  target_directory = paths$export_dir,
  to_zip = FALSE,
  debug = debug
)

size_data_csv <- fs::file_size(data_csv_path)

# Export "mc_data" to zip file (~10x less disk space for test dataset)
data_zip_path <- kwb.geosalz::write_measurementchains_data(
  mc_data,
  target_directory = paths$export_dir,
  to_zip = TRUE,
  debug = debug
)

size_data_zip <- fs::file_size(data_zip_path)
size_data_csv / size_data_zip

extract_data_timeperiod <- function(file) {
  basename(file) %>% 
    kwb.utils::replaceFileExtension("") %>%  
    stringr::str_remove("^mc[-|_]data")
}

# Define helper function to write a csv file in a target directory
write_csv <- function(df, postfix = "", target_dir) {
  fs::dir_create(target_dir)
  name <- deparse(substitute(df))
  file <- file.path(target_dir, paste0(name, postfix,  ".csv"))
  readr::write_csv(df, file)
  file
}

postfix <- extract_data_timeperiod(data_zip_path)
target_dir <- paths$export_dir

# Export data and metadata to csv files
stats_path <- write_csv(mc_data_stats, postfix, target_dir)
metadata_path <- write_csv(metadata, postfix, target_dir)
files_path <- write_csv(mc_files, postfix, target_dir)

# Filter out too high conductivity values (> 20000 uS)
mc_data_filtered <- mc_data %>% 
  dplyr::filter(!(parameter == "Leitfaehigkeit" & messwert > 20000))

# Define function that plots data to a pdf file
plot_to_pdf <- function(mc_data, para, target_dir = ".", debug = TRUE) {
  
  plots <- kwb.geosalz::plot_measurementchains(mc_data, para)
  
  sapply(names(plots), function(name) {
    path <-  file.path(target_dir, sprintf("mc_data_%s.pdf", name))
    kwb.utils::catAndRun(
      sprintf("Writting '%s' to '%s'", name, path),
      expr = {
        kwb.utils::preparePdf(path, width.cm = 25, height.cm = 15)
        on.exit(dev.off())
        print(plots[[name]])
        path
      }, 
      dbg = debug
    )
  })
}

pdf_files <- c(
  plot_to_pdf(mc_data_filtered, "Leitfaehigkeit", paths$export_dir), 
  plot_to_pdf(mc_data_filtered, "Temperatur", paths$export_dir)
)

data_zip_path
```

The file size of the `r basename(data_csv_path)` file is 
`r round(size_data_csv / size_data_zip, 1)` times larger compared to compressing 
it into `r basename(data_zip_path)`.

### Data Upload 

In order to automate the data management process the 
[exported data](#data-export) these data will be uploaded to a restricted 
shared folder `r paths$upload_directory` on the KWB cloud. For doing so the 
following environment variables need to be defined in case the code below should 
be run from a client computer. However, as these variables are defined in a 
GitHub actions workflow there is no need to do this locally.

```
NEXTCLOUD_URL = "https://<replace-with-kwb-cloud-url>"
NEXTCLOUD_USER = "<your-kwb-cloud-username>" # your username
NEXTCLOUD_PASSWORD = "your-nextcloud-app-password" ### see details below
```

Subsequently the following code is run for uploading the data:

```{r measurementchains_data-upload,  eval = con_defined & nc_defined}
# List paths of files to upload explicitly
upload_files <- kwb.geosalz:::exclude_missing_files(c(
  stats_path, 
  pdf_files,
  metadata_path, 
  files_path,
  data_zip_path
))

for (file in upload_files) {
  
  kwb.utils::catAndRun(
    messageText = paste("Uploading file", file),
    expr = try(kwb.nextcloud::upload_file(
      file = file, 
      target_path = paths$upload_dir
    )),
    dbg = TRUE
  )
}
```

```{r measurementchains_data-delete, echo = FALSE, eval = local}
files <- list.files(paths$export_dir, full.names = TRUE)

result <- try(fs::file_delete(path = files))

if (kwb.utils::isTryError(result)) {
  message(
    "Error when trying to delete these files:\n- ", 
    paste(basename(files), collapse = "\n- "),
    
  )
}
```

### Download Well Operation Data from Cloud 

```{r download_well_operation_data_from_cloud}

mc_dat <- mc_data_filtered %>%  
  dplyr::left_join(metadata[,c("sensor_id", "einbau_sensor_muGOK")], by = "sensor_id") %>% 
  dplyr::left_join(mc_files %>% dplyr::select(sftp_path, galerie, brunnen_nummer),
                   by = c(file = "sftp_path"))


well_op_file <- kwb.nextcloud::list_files(path = paths$well_operation,
                                          full_info = TRUE) %>% 
  dplyr::filter(lastmodified == max(lastmodified))


xlsx_file <- kwb.nextcloud::download_files(hrefs = well_op_file$href,
                                           target_dir = paths$export_dir)

well_op_data <- readxl::read_xlsx(path = xlsx_file) %>% 
  janitor::clean_names() %>%
  dplyr::filter(.data$menge_summe_m3 < 2000)



separate_name_der_messstelle_gms <- function(string) {
  
tibble::tibble(
  wasserwerk = stringr::str_sub(string, 1L, 3L),
  galerie = stringr::str_sub(string, 4L, 4L) %>% toupper(),
  brunnen_nummer = stringr::str_sub(string, 5L, 9L) %>%
    stringr::str_remove_all(pattern = "-") %>% 
    as.integer(),
  unbekannter_buchstabe = stringr::str_sub(string, 10L, 10L) %>%
    stringr::str_remove_all(pattern = "-") %>% 
    as.character(),
  brunnen_baujahr = stringr::str_sub(string, 12L, 15L) %>%
    stringr::str_remove_all(pattern = "-") %>% 
    as.integer(),
  brunnen_bauart = stringr::str_sub(string, 16L, 16L) %>%
    stringr::str_remove_all(pattern = "-") %>% 
    as.character()
  )
}


well_op_data_meta <- well_op_data %>% 
  dplyr::bind_cols(separate_name_der_messstelle_gms(well_op_data$name_der_messstelle_gms))


```


### Make combined EC and well operation plot

and upload on cloud.

```{r make_combined_plot,eval = TRUE}
well_ids <- c(9,10,13)

pdf_names <- sprintf("mc_and_q_well-%02d.pdf", well_ids)

target_dir <- "."
para <- "Leitfaehigkeit"
debug <- TRUE

### Make pdf for each well  
pdf_files <- sapply(well_ids, function(well_id) {
    path <-  file.path(target_dir, sprintf("mc-%s_and_abstraction_well-%02d.pdf", 
                                           para,
                                           well_id))
    kwb.utils::catAndRun(
      sprintf("Writting '%s' to '%s'", well_id, path),
      expr = {
        kwb.utils::preparePdf(path, landscape = TRUE)
        on.exit(dev.off())
        print(
          kwb.geosalz::plot_measurementchain_and_well_operation(
            mc_dat = mc_dat,
            well_op_data_meta = well_op_data_meta,
            brunnen_nr = well_id,
            para = para,
            date_min = as.Date("2023-05-10")))
        path
      }, 
      dbg = debug
    )
  })
  
### Upload pdf files on cloud 
  
  for (file in  pdf_files) {
  
  kwb.utils::catAndRun(
    messageText = paste("Uploading file", file),
    expr = try(kwb.nextcloud::upload_file(
      file = file, 
      target_path = paths$upload_dir
    )),
    dbg = TRUE
  )}

```